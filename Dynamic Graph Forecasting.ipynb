{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ba800-46ba-45a5-8278-da5a5031369e",
   "metadata": {},
   "source": [
    "# 6 Hour Forecast using Dynamic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f13f10-9486-4add-9131-88e445870233",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle # to access dataframe faster than csv\n",
    "import glob, re\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import scipy as sp\n",
    "import networkx\n",
    "import torch_geometric\n",
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "# for tweaked_TAGConv \n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm # adjacency matrix normalization\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import zeros # initialize weights and biases for nn\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
    "from torch_geometric.utils import spmm # for sparse matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d284e-0ef6-4fb7-93df-339b826c5c80",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "1. Aggregated (dataframe available at https://drive.google.com/drive/folders/1CwSLAJeCGUuHXRJOZ9YgkraHYaE8pGGH?usp=sharing)\n",
    "2. Load aggregated\n",
    "3. Load window csv with lclids corresponding to aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86dd50-97b4-44c4-a948-c7d9c69a6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "file = open('uk_smart_meter_aggregated/df_agg.pkl','rb')\n",
    "df_agg = pickle.load(file)\n",
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75bde78-3191-428e-87cd-ade22b841885",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('uk_smart_meter_aggregated/windows_agg_ids.pkl','rb')\n",
    "windows = pickle.load(file)\n",
    "windows.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abdb60-7ee3-46fd-8949-7ea559ec176d",
   "metadata": {},
   "source": [
    "## Snapshot-based temporal graph\n",
    "Idea: Build the adjacency matrix for all nodes using `create_adjacency_matrix`. Select a time-stamp for example [0]: '2012-01-01 00:00:00' or [1]: '2012-01-01 01:00:00' and so on. For this time-stamp what all nodes are active? Make an adjacency matrix for only these nodes for that particular time stamp using `get_snapshot_adjacency`. This function further returns the LCLids of active nodes (2,3,... 5564) and the indices of those active nodes (dataframe indices as in from range 0 to 5557)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b44c-3cf8-4da6-b510-d3fe6204e4f5",
   "metadata": {},
   "source": [
    "1. Get the adjacency matrix for *all the nodes* at once to avoid computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a0513-7deb-4a80-9893-c8f5575d670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(lclids, k):\n",
    "    number_of_nodes = sum([len(l) for l in lclids])\n",
    "    adjacency_matrix = np.zeros((number_of_nodes, number_of_nodes))\n",
    "    # Create the graph by iterating over the list of lists of LCLids\n",
    "    # and connecting all nodes in the list with each other\n",
    "    # and with the k-nearest lists\n",
    "    for i in range(len(lclids)): # range 2156\n",
    "        for j in range(len(lclids)): # range 2156 \n",
    "            if i == j: \n",
    "                for lclid in lclids[i]:\n",
    "                    for lclid2 in lclids[j]:\n",
    "                        adjacency_matrix[lclid, lclid2] = 1\n",
    "            elif abs(i-j) <= k:\n",
    "                for lclid in lclids[i]:\n",
    "                    for lclid2 in lclids[j]:\n",
    "                        adjacency_matrix[lclid, lclid2] = 1\n",
    "    adjacency_matrix = adjacency_matrix - np.eye(number_of_nodes)\n",
    "    return sp.sparse.bsr_array(adjacency_matrix)\n",
    "    \n",
    "# Sort rows by start date\n",
    "windows_copy = windows.sort_values(by='Enabled At').copy()\n",
    "\n",
    "# Get an ordered list of dates 'Enabled At'\n",
    "enable_unique_dates = windows_copy['Enabled At'].unique()\n",
    "\n",
    "# Get a list of lists of LCLids that have the same start date\n",
    "nbor_lclids = [windows_copy[windows_copy['Enabled At'] == date].index.tolist() for date in enable_unique_dates]\n",
    "print('Number of unique start dates: ', len(enable_unique_dates))\n",
    "assert len(windows) == sum([len(l) for l in nbor_lclids])\n",
    "\n",
    "# K-nearest neighbours\n",
    "k = 50\n",
    "\n",
    "adjacency_matrix = create_adjacency_matrix(nbor_lclids, k)\n",
    "# Compute the sparsity of the adjacency matrix\n",
    "sparsity = 1 - sp.sparse.bsr_matrix.count_nonzero(adjacency_matrix) / np.prod(adjacency_matrix.shape)\n",
    "G = networkx.from_scipy_sparse_array(adjacency_matrix)\n",
    "print(f'Graph connected for k={k}: {networkx.is_connected(G)}')\n",
    "print('sparsity: ', sparsity)\n",
    "\n",
    "plt.spy(sp.sparse.bsr_matrix.toarray(adjacency_matrix))\n",
    "plt.xlabel(\"Household mapped indices\"), plt.ylabel(\"Household mapped indices\")\n",
    "# save the adjacency matrix\n",
    "np.save('Results/adjacency_matrix.npy', adjacency_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbd1f7-e613-4520-ab5b-4d44fd0258e1",
   "metadata": {},
   "source": [
    "2. Make a function to get the sub-adjacency matrix for all nodes active at a given time instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174aceef-34e1-4d93-8b07-c733397c40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshot_adjacency(timestampidx, full_adjacency_matrix, df_agg):\n",
    "    \"\"\"\n",
    "    timestampidx: Time index; For example [0]: '2012-01-01 00:00:00'\n",
    "    full_adjacency_matrix (np.array()): adjacency matrix for all the LCLids\n",
    "    df_agg: aggregated dataframe with timeseries for all LCLids \n",
    "    \n",
    "    Returns:\n",
    "    indices_active_nodes: dataframe indices of active nodes\n",
    "    active nodes: LCLids of active nodes \n",
    "    active_sparse_submat: adjacency matrix obtained for that particular timestamp (using timestampidx)\n",
    "    \"\"\"\n",
    "    full_adjacency_matrix = full_adjacency_matrix.toarray()\n",
    "\n",
    "    # get all the active nodes for that particular time-stamp\n",
    "    active_nodes = df_agg.columns[df_agg.loc[df_agg.index[timestampidx], :].notna()]\n",
    "    \n",
    "    # indices of active nodes\n",
    "    indices_active_nodes = windows[windows['LCLid'].isin(active_nodes.values)].index\n",
    "\n",
    "    # active nodes sub-adjacency matrix\n",
    "    active_adj_submat = full_adjacency_matrix[np.ix_(indices_active_nodes, indices_active_nodes)]\n",
    "\n",
    "    if active_adj_submat.shape[0] != active_nodes.shape[0]:\n",
    "        print(f'# active nodes = {active_nodes.shape}, while \\\n",
    "        Adjacency Matrix Shape = {active_adj_submat.shape}')\n",
    "        raise RuntimeError()\n",
    "\n",
    "    # create graph from the adjacency submatrix to check if it is connected\n",
    "    active_sparse_submat = sp.sparse.bsr_array(active_adj_submat)\n",
    "    \n",
    "    G = networkx.from_scipy_sparse_array(active_sparse_submat)\n",
    "\n",
    "    # check if the graph is fully connected\n",
    "    assert networkx.is_connected(G)\n",
    "\n",
    "    # FOR FURTHER ANALYSIS\n",
    "    #sparsity_submat = 1 - sp.sparse.bsr_matrix.count_nonzero(active_sparse_submat) \\\n",
    "    #/ np.prod(active_sparse_submat.shape)\n",
    "    #print(f'Sparsity = {sparsity_submat}')\n",
    "    \n",
    "    \n",
    "    # get edge indices from the adjacency submatrix COO Format\n",
    "    edge_index = torch.tensor(np.array(G.edges).T)\n",
    "\n",
    "    # node feature matrix \n",
    "    node_feat = torch.tensor(df_agg.loc[df_agg.index[timestampidx],active_nodes].values).view(-1,1)\n",
    "    return indices_active_nodes, active_nodes, active_sparse_submat, node_feat, edge_index\n",
    "\n",
    "\n",
    "# example usage:\n",
    "indices, activenodes, active_sparse_submat, node_feat, edge_idx = get_snapshot_adjacency(0, adjacency_matrix, df_agg) # check exact date using df_agg.index[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a50be-dd79-45d0-97c7-92350a74bbfa",
   "metadata": {},
   "source": [
    "### Align snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac1e83-5911-43f1-8d4a-bc7fd320b8b0",
   "metadata": {},
   "source": [
    "Idea: Take $n$ snapshots and extract the sub-adjacency matrix for all these time-stamps. Make a product graph using spatial temporal coupling for all these snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b2f66-b769-4a87-996e-00bf44a2c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Snapshots taken at {df_agg.index[0]}, {df_agg.index[50]}, {df_agg.index[100]} to calculate the supra adjacency matrix')\n",
    "def get_subadj_book(window_idx, adjacency_matrix, df_agg):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        - window index (for eg., range(0,23) for first 24 hours)\n",
    "        - full adjacency matrix \n",
    "        - aggregated dataframe \n",
    "    Returns \n",
    "        - index (of dataframe) book of all active nodes throughout the window for all time instances\n",
    "        - LCLids of active nodes throughout the window for all time instances\n",
    "        - subadjacency matrix throughout the window for all time instances\n",
    "        - LCLids of all active unique nodes inside the window_idx\n",
    "    \"\"\"\n",
    "    node_feat_book = {}\n",
    "    edge_idx_book = {}\n",
    "    subadj_book = {}\n",
    "    idx_book = {}\n",
    "    actnod_book = {}\n",
    "    all_nodes = {}\n",
    "    for i, idx in enumerate(window_idx):\n",
    "        # extract the subadjacency matrix at i-th time-stamp\n",
    "        idx_i, actnod_i, subadj_i, node_feat_i, edge_idx_i = get_snapshot_adjacency(idx, adjacency_matrix, df_agg)\n",
    "        idx_book[i] = idx_i\n",
    "        actnod_book[i] = actnod_i\n",
    "        subadj_book[i] = subadj_i.toarray()\n",
    "        node_feat_book[i] = node_feat_i\n",
    "        edge_idx_book[i] = edge_idx_i\n",
    "        \n",
    "        # Union of nodes in all snapshots\n",
    "        # EX: snapshot 1 nodes: {3,2,1}; snapshot 2 nodes: {2,3,4}; snapshot 3 nodes: {1,5,6}\n",
    "        # all nodes = {3,2,1,4,5,6}\n",
    "        all_nodes = set(all_nodes).union(set(actnod_i))\n",
    "    return idx_book, actnod_book, subadj_book, all_nodes, node_feat_book, edge_idx_book\n",
    "\n",
    "window_idx = 100*np.array([0,1,2,3,4,5,6,7,8,9,10])# np.arange(24) #100*np.array([0,1,2,3,4,5,6,7,8,9,10]) # training window\n",
    "# extract the subadjacency matrix for all those time-stamps\n",
    "idx_book, actnod_book, subadj_book, all_nodes, node_feat_book, edge_idx_book\\\n",
    "= get_subadj_book(window_idx, adjacency_matrix, df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35506712-fd82-4308-98b6-ce6d2f8e9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74348fff-d87b-47b5-a0e1-102452446c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_adjacency_matrix(active_nodes, subadj, all_nodes, node_index_map):\n",
    "    if not isinstance(subadj, np.ndarray):\n",
    "        subadj = subadj.toarray()\n",
    "    aligned_subadj = np.zeros((len(all_nodes), len(all_nodes)))\n",
    "    for i, node_i in enumerate(active_nodes):\n",
    "        for j, node_j in enumerate(active_nodes):\n",
    "            if node_i in node_index_map and node_j in node_index_map:\n",
    "                idx_i = node_index_map[node_i]\n",
    "                idx_j = node_index_map[node_j]\n",
    "                aligned_subadj[idx_i, idx_j] = subadj[i, j]\n",
    "    return aligned_subadj\n",
    "\n",
    "\n",
    "def get_aligned_adj_book(subadj_book, node_feat_book, actnod_book, all_nodes):\n",
    "    \n",
    "    # create node index mapping\n",
    "    # for ex., index_mapping = {0,1,2,3,4,5} for sorted nodes {1,2,3,4,5,6}\n",
    "    node_index_map = {node: i for i, node in enumerate(sorted(all_nodes))} # node: idx dictionary\n",
    "\n",
    "    # total nodes \n",
    "    num_nodes = len(node_index_map)\n",
    "\n",
    "    aligned_adj_book = {}\n",
    "    aligned_node_feat_book = {}\n",
    "    aligned_edge_index_book = {}\n",
    "\n",
    "    for i, ((actkey, actnod), (subadjkey, subadj)) in enumerate(zip(actnod_book.items(), subadj_book.items())):\n",
    "        #print(actkey, actnod), print(subadjkey, subadj)\n",
    "        aligned_adj_book[i] = align_adjacency_matrix(actnod, subadj, all_nodes, node_index_map)\n",
    "        # G = networkx.from_scipy_sparse_array(sp.sparse.bsr_array(aligned_adj_book[i]))\n",
    "        # aligned_edge_index_book[i] = torch.tensor(np.array(G.edges).T)\n",
    "        \n",
    "        # Extract the indices of the non-zero elements (edges)\n",
    "        row_indices, col_indices = np.nonzero(aligned_adj_book[i])\n",
    "\n",
    "        # Combine the row and column indices to form the edge_index\n",
    "        aligned_edge_index_book[i] = torch.tensor(np.vstack((row_indices, col_indices)))\n",
    "        \n",
    "        for j, key in enumerate(actnod.values):\n",
    "            temp = np.zeros((len(all_nodes), 1))\n",
    "            temp[node_index_map[key]] = node_feat_book[actkey][j]\n",
    "        aligned_node_feat_book[i] = temp\n",
    "      \n",
    "    return aligned_book, node_index_map, aligned_node_feat_book, aligned_edge_index_book\n",
    "\n",
    "    \n",
    "\n",
    "aligned_adj_book, node_index_map, aligned_node_feat_book, aligned_edge_index_book = get_aligned_adj_book(subadj_book, node_feat_book, actnod_book, all_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf0b79-cbaf-4552-afa8-21cc3bf4a295",
   "metadata": {},
   "source": [
    "### Graph-VAR\n",
    "\n",
    "$$\n",
    "\\mathbf{x_t} = -\\sum_{p=1}^{P} \\sum_{k=0}^{K} h_{kp}\\mathbf{S}_{t-p}^k \\mathbf{x}_{t-p} \n",
    "$$\n",
    "\n",
    "where P in our case will be 24. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da97af-d2cd-497a-a831-d9f8c18140d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraphDataset: # Add data for training\n",
    "    def __init__(self):\n",
    "        self.graph_data = []\n",
    "\n",
    "    def add_time_instance(self, node_features, edge_index):\n",
    "        edge_index, edge_weight = gcn_norm(edge_index=edge_index.long(),\n",
    "                                           edge_weight=None, \n",
    "                                           num_nodes=node_features.shape[0],\n",
    "                                           add_self_loops=False)\n",
    "        data = Data(x=torch.tensor(node_features).float(), edge_index=edge_index.float(), edge_weight = edge_weight)\n",
    "        self.graph_data.append(data)\n",
    "\n",
    "    def get_time_instance(self, time):\n",
    "        return self.graph_data[time]\n",
    "        \n",
    "# Create the dataset\n",
    "dataset = TemporalGraphDataset()\n",
    "\n",
    "for i,_ in enumerate(window_idx):\n",
    "    # features\n",
    "    # x_idx = torch.tensor(df_agg.loc[df_agg.index[i],actnod_book[i]].values).view(-1,1)\n",
    "    dataset.add_time_instance(aligned_node_feat_book[i], aligned_edge_index_book[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e15d5-4df0-4389-8fff-a49f34a69624",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_time_instance(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe3a2f-51ae-4fae-9599-675db195531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGCNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int, P: int, normalize: bool = True): # P from window_idx = np.arange(P)\n",
    "        super(TemporalGCNLayer, self).__init__(aggr = 'add') # 'Add' aggregation\n",
    "        self.K = K\n",
    "        self.P = P\n",
    "        # self.normalize = normalize\n",
    "        # self.linear = nn.Linear(in_channels, out_channels)\n",
    "        self.h = nn.Parameter(torch.Tensor(K+1, P)) # depends on the order of filter and P\n",
    "        self.reset_parameters() # initialize parameters\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        #nn.init.xavier_uniform_(self.linear.weight) # initialize the weight of the linear layer\n",
    "        #nn.init.zeros_(self.linear.bias)\n",
    "        nn.init.xavier_uniform_(self.h)\n",
    "\n",
    "    def forward(self, dataset) -> Tensor:\n",
    "        out = torch.zeros_like(dataset.get_time_instance(0).x) # out dim = (410,1)\n",
    "        for p in range(0, self.P):      # 1 to P but indexing in python\n",
    "            for k in range(self.K + 1): # 0 to K\n",
    "                h_kp = self.h[k, p-1]   # access the [k, p-1] parameter from parameter matrix\n",
    "                edge_index = dataset.get_time_instance(p).edge_index.long()\n",
    "                edge_weight = dataset.get_time_instance(p).edge_weight\n",
    "                adj_sparse_tensor = SparseTensor(row = edge_index[0], col = edge_index[1], value=edge_weight)\n",
    "                # print(isinstance(adj_matrix, torch.FloatTensor))\n",
    "                x_t_minus_p = dataset.get_time_instance(p).x\n",
    "                out += h_kp * self.propagate(adj_sparse_tensor, x = x_t_minus_p, k = k)\n",
    "        return out # self.linear(out) # here equals out\n",
    "\n",
    "    def message(self, x_j, edge_weight): # not needed in our case\n",
    "        return edge_weight.view(-1,1) * x_j \n",
    "\n",
    "    def propagate(self, adj_sparse_tensor, x = None, k = 1):\n",
    "        x_out = x\n",
    "        for _ in range(k):\n",
    "            x_out = adj_sparse_tensor.matmul(x_out)\n",
    "        return x_out \n",
    "\n",
    "class TemporalGCN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, K, P):\n",
    "        super(TemporalGCN, self).__init__()\n",
    "        self.gcn_layer = TemporalGCNLayer(in_channels, out_channels, K, P)\n",
    "\n",
    "    def forward(self, dataset):\n",
    "        return self.gcn_layer(dataset)\n",
    "\n",
    "# Initialize the model\n",
    "in_channels = 1 # 1 feature\n",
    "out_channels = 1 # 1 feature\n",
    "K = 2 # order of polynomial filter\n",
    "P = len(window_idx)\n",
    "model = TemporalGCN(in_channels, out_channels, K, P)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd62447-2fbe-4e0f-b546-7600ee48b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of parameters to verify \n",
    "# for K = 2, P = 3, no. of parameters = 9\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_params = count_parameters(model)\n",
    "print(f'The model has {num_params} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6b978-38a6-4bc1-88c9-276d1b703889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(window_idx, all_nodes):\n",
    "    target_timestamp = window_idx[-1]\n",
    "    # get target values for all the nodes active for training \n",
    "    target = torch.tensor(df_agg.loc[df_agg.index[target_timestamp],list(all_nodes)].values).view(-1,1).float()\n",
    "    return target\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2000\n",
    "target = get_target(window_idx, all_nodes)\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass \n",
    "    predict = model(dataset)\n",
    "\n",
    "    loss = criterion(predict, target)\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Training Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6847275-1c8c-492a-94d7-349edaa8ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46300c-f1f7-4220-bdcb-490659d8f67e",
   "metadata": {},
   "source": [
    "### Build the supra-adjacency matrix $S_\\diamond$ from all the aligned subadjacency matrices \"book\" $\\$_i$ \n",
    "Build a supra-adjacency matrix for $n$ time instances using spatio-temporal coupling. Note: we cannot use the kronecker product here since at all instances, the adjacnecy matrix `aligned_subadj1/2/3` is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4febc29-ccba-4625-a1c0-fcf762ea3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supra_adjacency_matrix(aligned_subadj_book):\n",
    "\n",
    "    # convert the dict to stack \n",
    "    stacked_aligned_subadj = np.stack(list(aligned_book.values()), axis = -1)\n",
    "    \n",
    "    # time instances\n",
    "    instances = stacked_aligned_subadj.shape[2]\n",
    "\n",
    "    # size of aligned\n",
    "    a_size = stacked_aligned_subadj.shape[0]\n",
    "\n",
    "    # supra \n",
    "    S = np.zeros((a_size*instances, a_size*instances))\n",
    "\n",
    "    # Place aligned at appropriate locations for spatio-temporal coupling\n",
    "    # temporal + own-node spatial component\n",
    "    for i in range(1,instances):\n",
    "        S[i*a_size:(i+1)*a_size, (i-1)*a_size:i*a_size] = stacked_aligned_subadj[...,i-1] + np.eye(a_size) # last term is own-node spatial component\n",
    "    # spatial coupling\n",
    "    for i in range(instances):\n",
    "        S[i*a_size:(i+1)*a_size, i*a_size:(i+1)*a_size] = stacked_aligned_subadj[...,i]\n",
    "    \n",
    "    return S\n",
    "\n",
    "supra_adj_mat = supra_adjacency_matrix(aligned_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849704c-ad2a-4d0a-9028-66dd520ac2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.spy(supra_adj_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb5b21-4c05-4011-a545-96086d4d7382",
   "metadata": {},
   "source": [
    "### Imputation on Supra-adjacency matrix/Product Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708ab71-eedf-4401-b3ea-692ade3c84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_nodes)*len(window_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9215e-de17-42a0-8a4d-b64da5cf388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the y_diamond using data\n",
    "y_diamond = np.zeros([len(all_nodes)*len(actnod_book), 1])\n",
    "\n",
    "# the index with corresponding lclid will get the data\n",
    "\n",
    "# 6th LCLid \n",
    "y_diamond[0] = df_agg.loc[df_agg.index[window_idx[0]],list(all_nodes)[0]] \n",
    "# 16th LCLid\n",
    "y_diamond[1] = df_agg.loc[df_agg.index[window_idx[1]], list(all_nodes)[1]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
