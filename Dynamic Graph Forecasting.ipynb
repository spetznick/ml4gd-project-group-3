{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ba800-46ba-45a5-8278-da5a5031369e",
   "metadata": {},
   "source": [
    "# 6 Hour Forecast using Dynamic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f13f10-9486-4add-9131-88e445870233",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle # to access dataframe faster than csv\n",
    "import glob, re\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import scipy as sp\n",
    "import networkx\n",
    "import torch_geometric\n",
    "import torch\n",
    "from torch_geometric.utils.convert import from_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d284e-0ef6-4fb7-93df-339b826c5c80",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "1. Chunk Properly (dataframe available at https://drive.google.com/drive/folders/1CwSLAJeCGUuHXRJOZ9YgkraHYaE8pGGH?usp=sharing)\n",
    "2. Load aggregated\n",
    "3. Load window csv with lclids corresponding to aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86dd50-97b4-44c4-a948-c7d9c69a6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "file = open('uk_smart_meter_aggregated/df_agg.pkl','rb')\n",
    "df_agg = pickle.load(file)\n",
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75bde78-3191-428e-87cd-ade22b841885",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('uk_smart_meter_aggregated/windows_agg_ids.pkl','rb')\n",
    "windows = pickle.load(file)\n",
    "windows.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abdb60-7ee3-46fd-8949-7ea559ec176d",
   "metadata": {},
   "source": [
    "## Snapshot-based temporal graph\n",
    "Idea: Build the adjacency matrix for all nodes using `create_adjacency_matrix`. Select a time-stamp for example [0]: '2012-01-01 00:00:00' or [1]: '2012-01-01 01:00:00' and so on. For this time-stamp what all nodes are active? Make an adjacency matrix for only these nodes for that particular time stamp using `get_snapshot_adjacency`. This function further returns the LCLids of active nodes (2,3,... 5564) and the indices of those active nodes (dataframe indices as in from range 0 to 5557)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b44c-3cf8-4da6-b510-d3fe6204e4f5",
   "metadata": {},
   "source": [
    "1. Get the adjacency matrix for *all the nodes* at once to avoid computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a0513-7deb-4a80-9893-c8f5575d670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(lclids, k):\n",
    "    number_of_nodes = sum([len(l) for l in lclids])\n",
    "    adjacency_matrix = np.zeros((number_of_nodes, number_of_nodes))\n",
    "    # Create the graph by iterating over the list of lists of LCLids\n",
    "    # and connecting all nodes in the list with each other\n",
    "    # and with the k-nearest lists\n",
    "    for i in range(len(lclids)): # range 2156\n",
    "        for j in range(len(lclids)): # range 2156 \n",
    "            if i == j: \n",
    "                for lclid in lclids[i]:\n",
    "                    for lclid2 in lclids[j]:\n",
    "                        adjacency_matrix[lclid, lclid2] = 1\n",
    "            elif abs(i-j) <= k:\n",
    "                for lclid in lclids[i]:\n",
    "                    for lclid2 in lclids[j]:\n",
    "                        adjacency_matrix[lclid, lclid2] = 1\n",
    "    adjacency_matrix = adjacency_matrix - np.eye(number_of_nodes)\n",
    "    return sp.sparse.bsr_array(adjacency_matrix)\n",
    "    \n",
    "# Sort rows by start date\n",
    "windows_copy = windows.sort_values(by='Enabled At').copy()\n",
    "\n",
    "# Get an ordered list of dates 'Enabled At'\n",
    "enable_unique_dates = windows_copy['Enabled At'].unique()\n",
    "\n",
    "# Get a list of lists of LCLids that have the same start date\n",
    "nbor_lclids = [windows_copy[windows_copy['Enabled At'] == date].index.tolist() for date in enable_unique_dates]\n",
    "print('Number of unique start dates: ', len(enable_unique_dates))\n",
    "assert len(windows) == sum([len(l) for l in nbor_lclids])\n",
    "\n",
    "# K-nearest neighbours\n",
    "k = 50\n",
    "\n",
    "adjacency_matrix = create_adjacency_matrix(nbor_lclids, k)\n",
    "# Compute the sparsity of the adjacency matrix\n",
    "sparsity = 1 - sp.sparse.bsr_matrix.count_nonzero(adjacency_matrix) / np.prod(adjacency_matrix.shape)\n",
    "G = networkx.from_scipy_sparse_array(adjacency_matrix)\n",
    "print(f'Graph connected for k={k}: {networkx.is_connected(G)}')\n",
    "print('sparsity: ', sparsity)\n",
    "\n",
    "plt.spy(sp.sparse.bsr_matrix.toarray(adjacency_matrix))\n",
    "# save the adjacency matrix\n",
    "np.save('Results/adjacency_matrix.npy', adjacency_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbd1f7-e613-4520-ab5b-4d44fd0258e1",
   "metadata": {},
   "source": [
    "2. Make a function to get the sub-adjacency matrix for all nodes active at a given time instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174aceef-34e1-4d93-8b07-c733397c40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshot_adjacency(timestampidx, full_adjacency_matrix, df_agg):\n",
    "    \"\"\"\n",
    "    timestampidx: Time index; For example [0]: '2012-01-01 00:00:00'\n",
    "    full_adjacency_matrix (np.array()): adjacency matrix for all the LCLids\n",
    "    df_agg: aggregated dataframe with timeseries for all LCLids \n",
    "    \n",
    "    Returns:\n",
    "    indices_active_nodes: dataframe indices of active nodes\n",
    "    active nodes: LCLids of active nodes \n",
    "    active_sparse_submat: adjacency matrix obtained for that particular timestamp (using timestampidx)\n",
    "    \"\"\"\n",
    "    full_adjacency_matrix = full_adjacency_matrix.toarray()\n",
    "\n",
    "    # get all the active nodes for that particular time-stamp\n",
    "    active_nodes = df_agg.columns[df_agg.loc[df_agg.index[timestampidx], :].notna()]\n",
    "    \n",
    "    # indices of active nodes\n",
    "    indices_active_nodes = windows[windows['LCLid'].isin(active_nodes.values)].index\n",
    "\n",
    "    # active nodes sub-adjacency matrix\n",
    "    active_adj_submat = full_adjacency_matrix[np.ix_(indices_active_nodes, indices_active_nodes)]\n",
    "\n",
    "    if active_adj_submat.shape[0] != active_nodes.shape[0]:\n",
    "        print(f'# active nodes = {active_nodes.shape}, while \\\n",
    "        Adjacency Matrix Shape = {active_adj_submat.shape}')\n",
    "        raise RuntimeError()\n",
    "\n",
    "    # create graph from the adjacency submatrix to check if it is connected\n",
    "    active_sparse_submat = sp.sparse.bsr_array(active_adj_submat)\n",
    "    \n",
    "    G = networkx.from_scipy_sparse_array(active_sparse_submat)\n",
    "\n",
    "    # check if the graph is fully connected\n",
    "    assert networkx.is_connected(G)\n",
    "\n",
    "    # FOR FURTHER ANALYSIS\n",
    "    #sparsity_submat = 1 - sp.sparse.bsr_matrix.count_nonzero(active_sparse_submat) \\\n",
    "    #/ np.prod(active_sparse_submat.shape)\n",
    "    #print(f'Sparsity = {sparsity_submat}')\n",
    "    \n",
    "    \n",
    "    # get edge indices from the adjacency submatrix COO Format\n",
    "    # edge_index = torch.tensor(np.array(G.edges).T)\n",
    "\n",
    "    # node feature matrix \n",
    "    # x = torch.tensor(df_agg.loc[df_agg.index[100],activenodes].values).view(-1,1)\n",
    "    return indices_active_nodes, active_nodes, active_sparse_submat\n",
    "\n",
    "\n",
    "# example usage:\n",
    "indices, activenodes, active_sparse_submat = get_snapshot_adjacency(100, adjacency_matrix, df_agg) # check exact date using df_agg.index[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a50be-dd79-45d0-97c7-92350a74bbfa",
   "metadata": {},
   "source": [
    "## Produce Graph from Contiguous Snapshots - Supra Adjacency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac1e83-5911-43f1-8d4a-bc7fd320b8b0",
   "metadata": {},
   "source": [
    "Idea: Take $n$ snapshots and extract the sub-adjacency matrix for all these time-stamps. Make a product graph using spatial temporal coupling for all these snapshots. NEED A FUNCTION HERE TO AUTOMATE THE BELOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9bd178-7e91-472f-ba47-8fef6af40901",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Snapshots taken at {df_agg.index[0]}, {df_agg.index[50]}, {df_agg.index[100]} to calculate the supra adjacency matrix') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033fec2-1a89-4c59-b191-fb7c6233cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the subadjacency matrix for all those time-stamps\n",
    "idx1, actnod1, subadj1 = get_snapshot_adjacency(0, adjacency_matrix, df_agg) # subadj1 shape = (410,410)\n",
    "idx2, actnod2, subadj2 = get_snapshot_adjacency(500, adjacency_matrix, df_agg) # subadj2 shape = (491, 491)\n",
    "idx3, actnod3, subadj3 = get_snapshot_adjacency(1000, adjacency_matrix, df_agg) # subadj3 shape = (698, 698)\n",
    "\n",
    "# Union of nodes in all snapshots\n",
    "# EX: snapshot 1 nodes: {3,2,1}; snapshot 2 nodes: {2,3,4}; snapshot 3 nodes: {1,5,6}\n",
    "# all nodes = {3,2,1,4,5,6}\n",
    "all_nodes = set(actnod1).union(set(actnod2)).union(set(actnod3))\n",
    "\n",
    "# create node index mapping\n",
    "# index_mapping = {0,1,2,3,4,5} for sorted nodes {1,2,3,4,5,6}\n",
    "node_index_map = {node: i for i, node in enumerate(sorted(all_nodes))}\n",
    "\n",
    "# total nodes = 6\n",
    "num_nodes = len(node_index_map)\n",
    "\n",
    "# Align Adjacency Matrices\n",
    "# Construct aligned adjacency matrices for each snapshot\n",
    "# based on the union of all nodes.\n",
    "# By align I mean match the dimensions of all three snapshots. \n",
    "# snapshot 1: (410, 410) >> (698, 698), snapshot 2: (491, 491) >> (698, 698) \n",
    "# 698 is the number of union nodes which in our case is same as the number of active nodes of 3rd time-stamp\n",
    "def align_adjacency_matrix(active_nodes, subadj, all_nodes, node_index_map):\n",
    "    subadj = subadj.toarray()\n",
    "    aligned_subadj = np.zeros((len(all_nodes), len(all_nodes)))\n",
    "    for i, node_i in enumerate(active_nodes):\n",
    "        for j, node_j in enumerate(active_nodes):\n",
    "            if node_i in node_index_map and node_j in node_index_map:\n",
    "                idx_i = node_index_map[node_i]\n",
    "                idx_j = node_index_map[node_j]\n",
    "                aligned_subadj[idx_i, idx_j] = subadj[i, j]\n",
    "    return aligned_subadj\n",
    "\n",
    "aligned_subadj1 = align_adjacency_matrix(actnod1, subadj1, all_nodes, node_index_map)\n",
    "aligned_subadj2 = align_adjacency_matrix(actnod2, subadj2, all_nodes, node_index_map)\n",
    "aligned_subadj3 = align_adjacency_matrix(actnod3, subadj3, all_nodes, node_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46300c-f1f7-4220-bdcb-490659d8f67e",
   "metadata": {},
   "source": [
    "Build a supra-adjacency matrix for $n$ time instances using spatio-temporal coupling. Note: we cannot use the kronecker product here since at all instances, the adjacnecy matrix `aligned_subadj1/2/3` is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4febc29-ccba-4625-a1c0-fcf762ea3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supra_adjacency_matrix(stacked_aligned_subadj):\n",
    "    # time instances\n",
    "    instances = stacked_aligned_subadj.shape[2]\n",
    "\n",
    "    # size of aligned\n",
    "    a_size = stacked_aligned_subadj.shape[0]\n",
    "\n",
    "    # supra \n",
    "    S = np.zeros((a_size*instances, a_size*instances))\n",
    "\n",
    "    # Place aligned at appropriate locations for spatio-temporal coupling\n",
    "    # temporal + own-node spatial component\n",
    "    for i in range(1,instances):\n",
    "        S[i*a_size:(i+1)*a_size, (i-1)*a_size:i*a_size] = stacked_aligned_subadj[...,i-1] + np.eye(a_size) # last term is own-node spatial component\n",
    "    # spatial coupling\n",
    "    for i in range(instances):\n",
    "        S[i*a_size:(i+1)*a_size, i*a_size:(i+1)*a_size] = stacked_aligned_subadj[...,i]\n",
    "    \n",
    "    return S\n",
    "\n",
    "# Need functions for stacking...\n",
    "stacked_aligned_subadj = np.stack([aligned_subadj1, aligned_subadj2, aligned_subadj3], axis=-1)\n",
    "supra_adj_mat = supra_adjacency_matrix(stacked_aligned_subadj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f06a71-48a8-4dfb-bed2-119975972602",
   "metadata": {},
   "outputs": [],
   "source": [
    "supra_adj_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849704c-ad2a-4d0a-9028-66dd520ac2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(supra_adj_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5892079-a945-44a5-9fde-63258b272ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
